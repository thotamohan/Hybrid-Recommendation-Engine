{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "contentRecommendation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p40_f9bMqkNg"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz  \r\n",
        "!tar xf spark-2.3.0-bin-hadoop2.7.tgz  \r\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVEKfwPCqwnR"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.0-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KyEKC6HqzuU"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVzXUy-uq1m_"
      },
      "source": [
        "import itertools\r\n",
        "import pyspark\r\n",
        "import sys\r\n",
        "import time\r\n",
        "import json\r\n",
        "from pyspark import SparkContext, SparkConf\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql import SQLContext\r\n",
        "from pyspark.sql.functions  import date_format\r\n",
        "from pyspark.ml import Pipeline, PipelineModel\r\n",
        "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\r\n",
        "from pyspark.ml.feature import StopWordsRemover\r\n",
        "from pyspark.ml.feature import HashingTF, IDF\r\n",
        "from pyspark.ml.feature import Word2Vec, Word2VecModel\r\n",
        "from pyspark.ml.feature import CountVectorizer\r\n",
        "from pyspark.ml.feature import VectorAssembler\r\n",
        "from pyspark.ml.clustering import LDA, LDAModel, LocalLDAModel\r\n",
        "from wordcloud import WordCloud\r\n",
        "\r\n",
        "\r\n",
        "spark = SparkSession \\\r\n",
        "    .builder \\\r\n",
        "    .appName(\"Content Module\") \\\r\n",
        "    .getOrCreate()\r\n",
        "\r\n",
        "sqlContext = SQLContext(spark)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5r53PXvq386"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB3rV4h_q53E"
      },
      "source": [
        "class HybridRecommender:\r\n",
        "  this.maxCount=maxCount\r\n",
        "\r\n",
        "  output_path='/content/drive/MyDrive/'\r\n",
        "  model_path='/content/drive/MyDrive/'\r\n",
        "\r\n",
        "  def inputDataLoading(self):\r\n",
        "    /**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    */\r\n",
        "    #dataset loading\r\n",
        "\r\n",
        "    #user Dataframe Loading\r\n",
        "    user_df=spark.read.json('/content/drive/MyDrive/user.json')\r\n",
        "    user_avg_dict = spark.sparkContext.textFile('/content/drive/MyDrive/user_avg.json').persist().map(lambda x:json.loads(x)).take(1)\r\n",
        "    user_avg_dict= list(map(list, user_avg_dict[0].items()))\r\n",
        "    user_avg_df = spark.createDataFrame(user_avg_dict, [\"user_id\", \"average_stars\"])\r\n",
        "    user_df=user_df.join(user_avg_df,on='user_id',how='inner')\r\n",
        "\r\n",
        "    #business Dataframe loading\r\n",
        "    business_df=spark.read.json('/content/drive/MyDrive/business.json')\r\n",
        "    business_avg_dict = spark.sparkContext.textFile('/content/drive/MyDrive/business_avg.json').persist().map(lambda x:json.loads(x)).take(1)\r\n",
        "    business_avg_dict= list(map(list, business_avg_dict[0].items()))\r\n",
        "    business_avg_df = spark.createDataFrame(business_avg_dict, [\"business_id\", \"average_stars\"])\r\n",
        "    business_df=business_df.join(business_avg_df,on='business_id',how='inner')\r\n",
        "\r\n",
        "    #review Dataframe Loading\r\n",
        "    review_df=spark.read.json('train_review.json')\r\n",
        "\r\n",
        "    #preparing Dataframe for ALS Collaborative filtering\r\n",
        "    #adding userId integer column to business dataframe\r\n",
        "    b_idDict=business_df.select('business_id').rdd.map(lambda x:x[0]).zipWithIndex()\r\n",
        "    b_idDataFrame=sqlContext.createDataFrame(b_idDict,StructType([StructField(\"business_id\", StringType(), True),StructField(\"businessId\", IntegerType(), True)]))\r\n",
        "    a = business_df.alias(\"a\")\r\n",
        "    b = b_idDataFrame.alias(\"b\")\r\n",
        "    business_df = a.join(b, col(\"a.user_id\") == col(\"b.user_id\"), 'inner') \\\r\n",
        "                     .select([col('a.'+xx) for xx in a.columns] + [col('b.userId')])\r\n",
        "    HybridRecommender.business_df=business_df                    \r\n",
        "    \r\n",
        "    #adding userId integer column to user dataframe\r\n",
        "    u_idDict=user_df.select('user_id').rdd.map(lambda x:x[0]).zipWithIndex()\r\n",
        "    u_idDataFrame=sqlContext.createDataFrame(b_idDict,StructType([StructField(\"user_id\", StringType(), True),StructField(\"userId\", IntegerType(), True)]))\r\n",
        "    a = user_df.alias(\"a\")\r\n",
        "    b = u_idDataFrame.alias(\"b\")\r\n",
        "    user_df = a.join(b, col(\"a.user_id\") == col(\"b.user_id\"), 'inner') \\\r\n",
        "                     .select([col('a.'+xx) for xx in a.columns] + [col('b.userId')])\r\n",
        "    HybridRecommender.user_df=user_df \r\n",
        "\r\n",
        "    #adding both userId and businessId integer columns to review dataframe\r\n",
        "    a = review_df.alias(\"a\")\r\n",
        "    b = user_df.alias(\"b\")\r\n",
        "    review_df = a.join(b, col(\"a.user_id\") == col(\"b.user_id\"), 'inner') \\\r\n",
        "                     .select([col('a.'+xx) for xx in a.columns] + [col('b.userId')])\r\n",
        "\r\n",
        "    a = review_df.alias(\"a\")\r\n",
        "    b = business_df.alias(\"b\")\r\n",
        "    review_df = a.join(b, col(\"a.business_id\") == col(\"b.business_id\"), 'inner') \\\r\n",
        "                         .select([col('a.'+xx) for xx in a.columns] + [col('b.businessId')])\r\n",
        "    HybridRecommender.review_df=review_df                 \r\n",
        "\r\n",
        "    \r\n",
        "    #creating views to be used\r\n",
        "    business_df.createOrReplaceTempView(\"businesses\")\r\n",
        "    user_df.createOrReplaceTempView(\"users\")\r\n",
        "    review_df.createOrReplaceTempView(\"reviews\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P7KzA2Aq9eS"
      },
      "source": [
        "def textpreprocessing(self):\r\n",
        "  /**\r\n",
        "\r\n",
        "\r\n",
        "  */\r\n",
        "  review_df=review_df.rdd.map(lambda x:(x['business_id'],x['text'])).reduceByKey(add)\r\n",
        "  review_df=review_df.toDF().withColumnRenamed(\"_1\",\"business_id\").withColumnRenamed(\"_2\",\"reviewText\")\r\n",
        "  \r\n",
        "  regexTokenizer = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'reviewText', outputCol = 'generatedTokens')\r\n",
        "  stopWordsRemover = StopWordsRemover(inputCol = 'generatedTokens', outputCol = 'stopwordsRemoved')\r\n",
        "  countVectorizer = CountVectorizer(inputCol=\"stopwordsRemoved\", outputCol=\"VectorizedFeatures\")\r\n",
        "  iDF = IDF(inputCol=\"VectorizedFeatures\", outputCol=\"idfVector\")\r\n",
        "  word2Vec = Word2Vec(vectorSize = 100, minCount = 5, inputCol = 'stopwordsRemoved', outputCol = 'wordVectors', seed=123)\r\n",
        "  vectorAssembler = VectorAssembler(inputCols=['idfVector', 'wordVectors'], outputCol='combinedVectors')\r\n",
        "\r\n",
        "  pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, countVectorizer, iDF, word2Vec, vectorAssembler])\r\n",
        "  pipeline_mdl = pipeline.fit(review_df)\r\n",
        "  pipeline_mdl.write().overwrite().save('savedpipeLine_txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG1cq0buq-OM"
      },
      "source": [
        "def transformReviewText(self):\r\n",
        "  /**\r\n",
        "\r\n",
        "\r\n",
        "  */\r\n",
        "  pipeline_mdl = PipelineModel.load('savedpipeLine_txt')\r\n",
        "  review_df=review_df.rdd.map(lambda x:(x['business_id'],x['text'])).reduceByKey(add)\r\n",
        "  review_df=review_df.toDF().withColumnRenamed(\"_1\",\"business_id\").withColumnRenamed(\"_2\",\"reviewText\")\r\n",
        "  transformedReviewDF=pipeline_mdl.transform(review_df)\r\n",
        "  HybridRecommender.itemVectors=transformedReviewDF.rdd.map(lambda x:(x['business_id'],x['wordVectors'])).collect()\r\n",
        "  business_vectors = transformedReviewDF.select('business_id', 'wordVectors')\r\n",
        "  business_vectors.write.mode('overwrite').parquet('businessVectors.parquet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ_bHZTkrBm7"
      },
      "source": [
        "def cosineSimilarity(v1,v2):\r\n",
        "    '''\r\n",
        "    function explaination\r\n",
        "    similarity bet\r\n",
        "    userInput\r\n",
        "    return\r\n",
        "    '''\r\n",
        "    return float(np.dot(v1,v2)/(np.sqrt(np.dot(v1,v1))*(np.sqrt(np.dot(v2,v2)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNenXuacrE05"
      },
      "source": [
        "def businessDetails(input_business):\r\n",
        "    '''\r\n",
        "    function explaination\r\n",
        "    '''\r\n",
        "    inputDF=input_business.alias(\"input\")\r\n",
        "    businessDF=business_df.alias(\"businessDF\")\r\n",
        "    df=inputDF.join(businessDF,col(\"inputDF.business_id\")=col(\"businessDF.business_id\"),inner)\r\n",
        "    df= df.select([col('inputDF.'+xx) for xx in inputDF.columns] + [col('businessDF.business_name'),col('businessDF.categories'),\r\n",
        "                                                           col('businessDF.stars'),col('businessDF.review_count'),\r\n",
        "                                                           col('businessDF.latitude'),col('businessDF.longitude')])\r\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkDldJGXrIrq"
      },
      "source": [
        "def contentbasedRecommendations(self,userId,recommendationsCount=15):\r\n",
        "  /**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  */\r\n",
        "  query=\"\"\"\r\n",
        "  SELECT distinct business_id from reviews\r\n",
        "  where stars>=3.0 and user_id=\"{}\"\r\n",
        "  \"\"\".format(userId)\r\n",
        "  userRecommendedBusinesses= sqlContext.sql(query)\r\n",
        "  userRecommendedBusinesses = userRecommendedBusinesses.sample(False, 0.5).limit(5)\r\n",
        "  businessIds=[]\r\n",
        "  for ids in businessIds.collect():\r\n",
        "    businessIds.append(ids)\r\n",
        "  \r\n",
        "  allbusinesswordVectors=HybridRecommender.itemVectors\r\n",
        "  #extracting word vector(of review contents) for the business Vector\r\n",
        "  for businessId in businessIds:\r\n",
        "    wordVector=[vector[1] for vector in allbusinesswordVectors if vector[0]==businessId]\r\n",
        "    similarBusinesses=[]\r\n",
        "    #finding cosine similarities with respect to each business id\r\n",
        "    for bid in allbusinesswordVectors:\r\n",
        "      similarBusinesses.append((bid[0],float(cosineSimilarity(wordVector,bid[1]))))\r\n",
        "    similarbusinessRDD=sc.parallelize(similarBusinesses)\r\n",
        "    #sorting businessIds by cosine similarity values\r\n",
        "    similarbusinessRDD.sortBy(lambda a: -a[1])\r\n",
        "    similarbusinessRDD=similarbusinessRDD.filter(lambda x:x[0]!=businessId).take(recommendationsCount)\r\n",
        "\r\n",
        "    #removing businessIds already reviewed by given userId\r\n",
        "    a = similarbusinessRDD.alias(\"a\")\r\n",
        "    b = userRecommendedBusinesses.alias(\"b\")\r\n",
        "    bidScoreDF = s.join(r, col(\"a.business_id\") == col(\"b.business_id\"), 'left_outer').where(col(\"b.business_id\").isNull()) \\\r\n",
        "             .select([col('a.business_id'),col('b.score')])    \r\n",
        "    bidScoreDF = bidScoreDF.groupby('business_id').agg(max('score').alias('score'))\r\n",
        "    sortedbIDS = bidScoreDF.orderBy(\"score\", ascending = False).limit(max_recoms)\r\n",
        "    recommendedDF = self.businessDetails(sortedbIDS)\r\n",
        "  return DF"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}